export const maxDuration = 300

import { NextRequest, NextResponse } from 'next/server'
import { callLLM, buildTextMessages, buildMessagesWithImages, type ImageContent } from '@/lib/api-clients'
import { getBaremePrompt, BAREME_JSON_SCHEMA } from '@/lib/prompts'
import { robustJsonParse, normalizeBareme } from '@/lib/json-utils'
import { logLLMCall } from '@/lib/llm-logger'

// Mod√®les Anthropic qui supportent le prefilling du message assistant
// Opus 4.6 a l'adaptive thinking par d√©faut ‚Üí interdit le prefill
const ANTHROPIC_PREFILL_MODELS = new Set(['claude-haiku-4-5', 'claude-sonnet-4-5'])

// Mod√®les qui supportent le JSON mode natif (response_format ou responseMimeType)
const JSON_MODE_PROVIDERS = new Set(['openai', 'openai-responses', 'google', 'deepseek', 'xai', 'moonshot'])

// Prompt pour transcrire des images de document en texte
// Contexte √©ducatif explicite pour √©viter le filtre RECITATION de Gemini
const TRANSCRIPTION_DOC_PROMPT = `Tu es un assistant p√©dagogique qui aide un enseignant √† num√©riser ses documents de cours.
L'enseignant te fournit des photos d'un document scolaire (contr√¥le, exercice, ou corrig√©) qu'il a lui-m√™me r√©dig√©.
Ton r√¥le est de produire une version texte structur√©e de ce document pour qu'il puisse l'utiliser dans son outil de correction.

Instructions :
- Restitue le contenu complet du document en Markdown.
- Conserve la structure originale (titres, num√©rotation, sous-parties, consignes).
- Sois fid√®le au contenu : n'invente rien, ne r√©sume pas.
- Si un passage est difficile √† lire, fais de ton mieux et signale les incertitudes avec [illisible].

Pour les √©l√©ments visuels (graphiques, sch√©mas, figures, cartes, diagrammes) :
- D√©cris-les en d√©tail entre balises [FIGURE: ...]
- Pr√©cise : type de visuel, axes et l√©gendes, valeurs chiffr√©es, formes g√©om√©triques, relations spatiales, couleurs significatives
- Exemple : [FIGURE: Graphique en barres montrant la temp√©rature moyenne (¬∞C) en ordonn√©e et les mois (Jan-D√©c) en abscisse. Valeurs : Jan=5, F√©v=6, Mar=10, Avr=13, Mai=17, Jun=21, Jul=24, Ao√ª=23, Sep=20, Oct=15, Nov=9, D√©c=6]
- Exemple : [FIGURE: Triangle ABC rectangle en A. AB = 5 cm, AC = 12 cm. Angle B marqu√© Œ±. Hauteur AH trac√©e vers BC.]

Pour les tableaux, utilise la syntaxe Markdown de tableau.
Pour les formules math√©matiques, utilise la notation LaTeX entre $ ou $$.`

export async function POST (req: NextRequest) {
  const t0 = performance.now()
  const log = (step: string) => {
    const elapsed = ((performance.now() - t0) / 1000).toFixed(1)
    console.log(`[BAR√àME] ${elapsed}s ‚Äî ${step}`)
  }

  try {
    const body = await req.json()
    const { modelId, matiere, classe, enonceImages, corrigeImages, enonceText: existingEnonceText, corrigeText: existingCorrigeText } = body

    if (!modelId || !matiere || !classe || !enonceImages?.length) {
      return NextResponse.json(
        { error: 'Param√®tres manquants' },
        { status: 400 }
      )
    }

    log(`D√©but ‚Äî mod√®le=${modelId}, ${enonceImages.length} img √©nonc√©, ${corrigeImages?.length ?? 0} img corrig√©`)

    const env = {
      OPENAI_API_KEY: process.env.OPENAI_API_KEY,
      ANTHROPIC_API_KEY: process.env.ANTHROPIC_API_KEY,
      GOOGLE_API_KEY: process.env.GOOGLE_API_KEY,
      DEEPSEEK_API_KEY: process.env.DEEPSEEK_API_KEY,
      MISTRAL_API_KEY: process.env.MISTRAL_API_KEY,
      MOONSHOT_API_KEY: process.env.MOONSHOT_API_KEY,
      XAI_API_KEY: process.env.XAI_API_KEY,
    }

    // ‚îÄ‚îÄ‚îÄ Pr√©parer les images ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const enonceImgParsed: ImageContent[] = []
    for (const img of enonceImages) {
      const match = img.match(/^data:([^;]+);base64,(.+)$/)
      if (match) {
        enonceImgParsed.push({ mimeType: match[1], base64: match[2] })
      }
    }

    const corrigeImgParsed: ImageContent[] = []
    if (corrigeImages?.length) {
      for (const img of corrigeImages) {
        const match = img.match(/^data:([^;]+);base64,(.+)$/)
        if (match) {
          corrigeImgParsed.push({ mimeType: match[1], base64: match[2] })
        }
      }
    }

    const allImages = [...enonceImgParsed, ...corrigeImgParsed]
    const totalImgSize = allImages.reduce((sum, img) => sum + img.base64.length, 0)
    log(`${allImages.length} images pr√©par√©es (${(totalImgSize / 1024).toFixed(0)} KB base64)`)

    const provider = getProviderFromModel(modelId)
    const jsonMode = JSON_MODE_PROVIDERS.has(provider)
    const usePrefill = provider === 'anthropic' && ANTHROPIC_PREFILL_MODELS.has(modelId)

    // Anthropic structured outputs : sch√©ma JSON garanti pour Opus 4.6+
    const useStructuredOutput = provider === 'anthropic' && !ANTHROPIC_PREFILL_MODELS.has(modelId)

    // ‚îÄ‚îÄ‚îÄ D√©terminer si on travaille en mode texte ou images ‚îÄ‚îÄ
    const hasEnonceText = !!existingEnonceText
    const hasCorrigeText = !!existingCorrigeText
    const useTextMode = hasEnonceText

    if (useTextMode) {
      log('Mode TEXTE ‚Äî transcriptions disponibles, pas d\'envoi d\'images')
    } else {
      log('Mode IMAGES ‚Äî pas de transcription, envoi des images en multimodal')
    }

    // ‚îÄ‚îÄ‚îÄ Construire le prompt bar√®me ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const enonceContent = useTextMode ? existingEnonceText : '[Voir images de l\'√©nonc√© ci-jointes]'
    const corrigeContent = useTextMode && hasCorrigeText
      ? existingCorrigeText
      : (!useTextMode && corrigeImgParsed.length > 0)
        ? '[Voir images du corrig√© ci-jointes]'
        : undefined

    const prompt = getBaremePrompt(matiere, classe, enonceContent, corrigeContent ?? undefined)
    log(`Prompt construit (${prompt.length} chars) ‚Äî mode ${useTextMode ? 'texte' : 'images'}`)

    // Options LLM communes
    const llmOptions: Record<string, any> = { jsonMode }
    if (useStructuredOutput) {
      llmOptions.anthropicSchema = BAREME_JSON_SCHEMA
      log('Structured outputs activ√©s (Anthropic json_schema)')
    }

    // ‚îÄ‚îÄ‚îÄ G√©n√©ration du bar√®me ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const baremePromise = (async () => {
      let result: string

      if (useTextMode) {
        // Mode texte : prompt seul, pas d'images
        const messages = buildTextMessages('', prompt)

        if (usePrefill) {
          messages.push({ role: 'assistant', content: '{' })
        }

        result = await callLLM(modelId, messages, env, llmOptions)

        if (usePrefill && !result.startsWith('{')) {
          result = '{' + result
        }
      } else if (provider === 'google') {
        const parts = buildMessagesWithImages(prompt, allImages, modelId)
        result = await callLLM(modelId, parts, env, llmOptions)
      } else if (allImages.length > 0 && provider !== 'deepseek') {
        const messages = buildMessagesWithImages(prompt, allImages, modelId)

        if (usePrefill) {
          messages.push({ role: 'assistant', content: '{' })
        }

        result = await callLLM(modelId, messages, env, llmOptions)

        if (usePrefill && !result.startsWith('{')) {
          result = '{' + result
        }
      } else {
        const messages = buildTextMessages('', prompt)

        if (usePrefill) {
          messages.push({ role: 'assistant', content: '{' })
        }

        result = await callLLM(modelId, messages, env, llmOptions)

        if (usePrefill && !result.startsWith('{')) {
          result = '{' + result
        }
      }

      return result
    })()

    // ‚îÄ‚îÄ‚îÄ Transcriptions en parall√®le (si pas d√©j√† faites) ‚îÄ‚îÄ
    // Si on est en mode texte, pas besoin de re-transcrire
    const enonceTextPromise = existingEnonceText
      ? Promise.resolve(existingEnonceText as string)
      : transcribeDocImages(enonceImgParsed, env, log, '√©nonc√©')

    const corrigeTextPromise = existingCorrigeText
      ? Promise.resolve(existingCorrigeText as string)
      : corrigeImgParsed.length > 0
        ? transcribeDocImages(corrigeImgParsed, env, log, 'corrig√©')
        : Promise.resolve(null)

    // Attendre les r√©sultats en parall√®le
    const [baremeResult, enonceText, corrigeText] = await Promise.all([
      baremePromise,
      enonceTextPromise,
      corrigeTextPromise,
    ])

    log('R√©ponses re√ßues, parsing JSON du bar√®me...')
    const elapsedMs = performance.now() - t0

    // Parsing robuste + normalisation
    const parsed = robustJsonParse(baremeResult)
    const bareme = normalizeBareme(parsed)

    // Si aucune question n'a pu √™tre extraite, cr√©er un bar√®me minimal √©ditable
    if (bareme.questions.length === 0) {
      log('‚ö†Ô∏è Aucune question extraite ‚Üí bar√®me par d√©faut')
      bareme.total = 20
      bareme.questions = [
        {
          id: '1',
          titre: 'Item 1 ‚Äî √Ä compl√©ter',
          points: 20,
          criteres: [{ question: '', description: 'Crit√®re √† d√©finir par le professeur', points: 20 }],
        },
      ]
    }

    // Log asynchrone (non bloquant) ‚Äî prompt complet + r√©ponse brute
    const logContent = useTextMode
      ? prompt
      : `${prompt}\n\n[${allImages.length} images jointes ‚Äî non incluses dans le log]`
    logLLMCall({
      type: 'bareme',
      model: modelId,
      provider,
      prompt: { full: prompt },
      messages: [{ role: 'user', content: logContent }],
      options: { jsonMode, prefill: usePrefill },
      response_raw: baremeResult,
      response_parsed: bareme,
      meta: {
        elapsed_ms: Math.round(elapsedMs),
        timestamp: new Date().toISOString(),
        mode: useTextMode ? 'text' : 'images',
        images_count: useTextMode ? 0 : allImages.length,
        images_size_kb: useTextMode ? 0 : Math.round(totalImgSize / 1024),
      },
    })

    log(`‚úÖ Termin√© ‚Äî ${bareme.questions.length} sections, ${bareme.total} pts`)
    if (enonceText) log(`üìù √ânonc√© transcrit (${enonceText.length} chars)`)
    if (corrigeText) log(`üìù Corrig√© transcrit (${corrigeText.length} chars)`)

    return NextResponse.json({ bareme, enonceText, corrigeText })
  } catch (err: unknown) {
    const elapsed = ((performance.now() - t0) / 1000).toFixed(1)
    console.error(`[BAR√àME] ‚ùå ${elapsed}s ‚Äî Erreur:`, err)
    const message = err instanceof Error ? err.message : 'Erreur inconnue'
    return NextResponse.json({ error: message }, { status: 500 })
  }
}

/**
 * Transcrit des images de document en texte via Gemini 3 Flash (rapide, multimodal, pas cher).
 * Retourne null en cas d'erreur (non bloquant pour le bar√®me).
 */
async function transcribeDocImages (
  images: ImageContent[],
  env: Record<string, string | undefined>,
  log: (s: string) => void,
  label: string
): Promise<string | null> {
  try {
    const messages = buildMessagesWithImages(TRANSCRIPTION_DOC_PROMPT, images, 'gemini-3-flash')
    const result = await callLLM('gemini-3-flash', messages, env)
    log(`‚úÖ Transcription ${label} termin√©e`)
    return result
  } catch (err) {
    log(`‚ö†Ô∏è Transcription ${label} √©chou√©e (non bloquant) : ${err instanceof Error ? err.message : 'erreur'}`)
    return null
  }
}

function getProviderFromModel (modelId: string): string {
  const map: Record<string, string> = {
    'gpt-4o-mini': 'openai',
    'gpt-5-nano': 'openai',
    'gpt-5.2-pro': 'openai-responses',
    'gpt-5.2': 'openai',
    'claude-haiku-4-5': 'anthropic',
    'claude-sonnet-4-5': 'anthropic',
    'claude-opus-4-6': 'anthropic',
    'gemini-3-flash': 'google',
    'gemini-3-pro': 'google',
    'deepseek-v3.2': 'deepseek',
    'kimi-k2.5': 'moonshot',
    'kimi-k2-thinking': 'moonshot',
    'grok-4': 'xai',
  }
  return map[modelId] || 'openai'
}
