export const maxDuration = 300

import { NextRequest, NextResponse } from 'next/server'
import { callLLM, buildCorrectionMessages } from '@/lib/api-clients'
import { getCorrectionPromptParts, CORRECTION_JSON_SCHEMA } from '@/lib/prompts'
import { robustJsonParse, normalizeCorrection } from '@/lib/json-utils'
import { logLLMCall } from '@/lib/llm-logger'

// Mod√®les qui supportent le JSON mode natif (response_format ou responseMimeType)
const JSON_MODE_PROVIDERS = new Set(['openai', 'openai-responses', 'google', 'deepseek', 'xai', 'moonshot', 'mistral-chat'])

// Fallbacks par mod√®le : si le mod√®le principal √©choue, on essaie les suivants
const FALLBACK_MODELS: Record<string, string[]> = {
  'deepseek-v3.2': ['mistral-large', 'gemini-3-pro'],
}

export async function POST (req: NextRequest) {
  const t0 = performance.now()
  const log = (step: string) => {
    const elapsed = ((performance.now() - t0) / 1000).toFixed(1)
    console.log(`[CORRECTION] ${elapsed}s ‚Äî ${step}`)
  }

  try {
    const body = await req.json()
    const { modelId, matiere, classe, severite, baremeJson, mdCopie, enonceText, corrigeText } = body

    if (!modelId || !matiere || !classe || !severite || !baremeJson || !mdCopie) {
      return NextResponse.json(
        { error: 'Param√®tres manquants' },
        { status: 400 }
      )
    }

    log(`D√©but ‚Äî mod√®le=${modelId}, copie=${mdCopie.length} chars, bar√®me=${baremeJson.length} chars`)
    if (enonceText) log(`üìù √ânonc√© fourni (${enonceText.length} chars) ‚Äî sera mis en cache`)
    if (corrigeText) log(`üìù Corrig√© fourni (${corrigeText.length} chars) ‚Äî sera mis en cache`)

    const env = {
      OPENAI_API_KEY: process.env.OPENAI_API_KEY,
      ANTHROPIC_API_KEY: process.env.ANTHROPIC_API_KEY,
      GOOGLE_API_KEY: process.env.GOOGLE_API_KEY,
      DEEPSEEK_API_KEY: process.env.DEEPSEEK_API_KEY,
      MISTRAL_API_KEY: process.env.MISTRAL_API_KEY,
      MOONSHOT_API_KEY: process.env.MOONSHOT_API_KEY,
      XAI_API_KEY: process.env.XAI_API_KEY,
    }

    // ‚îÄ‚îÄ‚îÄ S√©parer le prompt en parties cacheable / variable ‚îÄ‚îÄ‚îÄ
    const { staticContext, variableContext } = getCorrectionPromptParts(
      matiere,
      classe,
      severite,
      baremeJson,
      mdCopie,
      enonceText || undefined,
      corrigeText || undefined
    )

    log(`Prompt construit ‚Äî statique: ${staticContext.length} chars (cacheable), variable: ${variableContext.length} chars`)

    // ‚îÄ‚îÄ‚îÄ Pipeline de correction avec fallback ‚îÄ‚îÄ‚îÄ
    const modelsToTry = [modelId, ...(FALLBACK_MODELS[modelId] || [])]
    const errors: string[] = []

    for (const currentModel of modelsToTry) {
      try {
        const messages = buildCorrectionMessages(staticContext, variableContext, currentModel)
        const provider = getProviderFromModel(currentModel)
        const jsonMode = JSON_MODE_PROVIDERS.has(provider)
        const useStructuredOutput = provider === 'anthropic' && currentModel !== 'claude-opus-4-6'

        const llmOptions: Record<string, any> = { jsonMode }
        if (useStructuredOutput) {
          llmOptions.anthropicSchema = CORRECTION_JSON_SCHEMA
          log('Structured outputs activ√©s (Anthropic json_schema)')
        }

        log(`Appel LLM ${currentModel}...`)
        const result = await callLLM(currentModel, messages, env, llmOptions)
        const elapsedMs = performance.now() - t0

        log('R√©ponse LLM re√ßue, parsing JSON...')

        const parsed = robustJsonParse(result)
        const correction = normalizeCorrection(parsed, baremeJson)

        // Log asynchrone (non bloquant)
        logLLMCall({
          type: 'correction',
          model: currentModel,
          provider,
          prompt: { static: staticContext, variable: variableContext },
          messages,
          options: { jsonMode },
          response_raw: result,
          response_parsed: correction,
          meta: {
            elapsed_ms: Math.round(elapsedMs),
            timestamp: new Date().toISOString(),
            failed_models: errors.length > 0 ? errors : undefined,
          },
        })

        if (!correction || correction.questions.length === 0) {
          const msg = `${currentModel}: correction non exploitable`
          log(`‚ö†Ô∏è ${msg}`)
          errors.push(msg)
          continue
        }

        if (currentModel !== modelId) {
          log(`‚úÖ Termin√© via fallback ${currentModel} ‚Äî note=${correction.note_globale}/${correction.total}, ${correction.questions.length} questions`)
        } else {
          log(`‚úÖ Termin√© ‚Äî note=${correction.note_globale}/${correction.total}, ${correction.questions.length} questions`)
        }
        return NextResponse.json({ correction, model: currentModel })
      } catch (err) {
        const msg = `${currentModel}: ${err instanceof Error ? err.message : 'Erreur inconnue'}`
        log(`‚ö†Ô∏è √âchec ${msg}`)
        errors.push(msg)
      }
    }

    // Tous les mod√®les ont √©chou√©
    log('‚ùå Tous les mod√®les ont √©chou√©')
    return NextResponse.json(
      { error: `Correction impossible :\n${errors.join('\n')}` },
      { status: 500 }
    )
  } catch (err: unknown) {
    const elapsed = ((performance.now() - t0) / 1000).toFixed(1)
    console.error(`[CORRECTION] ‚ùå ${elapsed}s ‚Äî Erreur:`, err)
    const message = err instanceof Error ? err.message : 'Erreur inconnue'
    return NextResponse.json({ error: message }, { status: 500 })
  }
}

function getProviderFromModel (modelId: string): string {
  const map: Record<string, string> = {
    'gpt-4o-mini': 'openai',
    'gpt-5-nano': 'openai',
    'gpt-5.2-pro': 'openai-responses',
    'gpt-5.2': 'openai',
    'claude-haiku-4-5': 'anthropic',
    'claude-sonnet-4-5': 'anthropic',
    'claude-opus-4-6': 'anthropic',
    'gemini-3-flash': 'google',
    'gemini-3-pro': 'google',
    'deepseek-v3.2': 'deepseek',
    'kimi-k2.5': 'moonshot',
    'kimi-k2-thinking': 'moonshot',
    'grok-4': 'xai',
    'mistral-large': 'mistral-chat',
  }
  return map[modelId] || 'openai'
}
