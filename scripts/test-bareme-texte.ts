#!/usr/bin/env bun
/* eslint-disable @typescript-eslint/no-explicit-any */

/**
 * Test barÃ¨me en mode TEXTE â€” transcription Gemini + test sur tous les LLM
 *
 * 1. Transcrit les images Ã©noncÃ© (1-4) via Gemini â†’ Ã©noncÃ©s/enonce.md
 * 2. Transcrit les images corrigÃ© (1-3) via Gemini â†’ Ã©noncÃ©s/corrige.md
 * 3. Lance la gÃ©nÃ©ration de barÃ¨me sur TOUS les LLM avec le texte transcrit
 *
 * Usage :
 *   bun run scripts/test-bareme-texte.ts
 *   bun run scripts/test-bareme-texte.ts --skip-transcription   # rÃ©utiliser les .md existants
 *   bun run scripts/test-bareme-texte.ts --model claude-opus-4-6
 */

import { readFileSync, writeFileSync, mkdirSync, existsSync } from 'fs'
import { join } from 'path'
import { robustJsonParse, normalizeBareme } from '../lib/json-utils'
import { getBaremePrompt } from '../lib/prompts'

// â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const SKIP_TRANSCRIPTION = process.argv.includes('--skip-transcription')
const MODEL_FILTER = getArgValue('--model')

function getArgValue (flag: string): string | null {
  const idx = process.argv.indexOf(flag)
  return idx !== -1 && idx + 1 < process.argv.length ? process.argv[idx + 1] : null
}

const ENV = {
  OPENAI_API_KEY: process.env.OPENAI_API_KEY || '',
  ANTHROPIC_API_KEY: process.env.ANTHROPIC_API_KEY || '',
  GOOGLE_API_KEY: process.env.GOOGLE_API_KEY || '',
  DEEPSEEK_API_KEY: process.env.DEEPSEEK_API_KEY || '',
  MISTRAL_API_KEY: process.env.MISTRAL_API_KEY || '',
  MOONSHOT_API_KEY: process.env.MOONSHOT_API_KEY || '',
  XAI_API_KEY: process.env.XAI_API_KEY || '',
}

const IMG_DIR = join(process.cwd(), 'Ã©noncÃ©s')
const ENONCE_MD = join(IMG_DIR, 'enonce.md')
const CORRIGE_MD = join(IMG_DIR, 'corrige.md')

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface TestResult {
  model: string
  status: 'ok' | 'ko'
  time: number
  questions: number
  total: number
  detail: string
  rawLength: number
  thinkingInfo?: string
}

const results: TestResult[] = []

// â”€â”€â”€ Fetch helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function fetchApi (url: string, body: any, headers: Record<string, string>, label: string): Promise<any> {
  const res = await fetch(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json', ...headers },
    body: JSON.stringify(body),
  })
  if (!res.ok) {
    const err = await res.text()
    throw new Error(`${label} ${res.status}: ${err.slice(0, 500)}`)
  }
  return res.json()
}

// â”€â”€â”€ Step 1 & 2 : Transcription via Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const TRANSCRIPTION_PROMPT = `Tu es un assistant pÃ©dagogique qui aide un enseignant Ã  numÃ©riser ses documents de cours.
L'enseignant te fournit des photos d'un document scolaire (contrÃ´le, exercice, ou corrigÃ©) qu'il a lui-mÃªme rÃ©digÃ©.
Ton rÃ´le est de produire une version texte structurÃ©e de ce document pour qu'il puisse l'utiliser dans son outil de correction.

Instructions :
- Restitue le contenu complet du document en Markdown.
- Conserve la structure originale (titres, numÃ©rotation, sous-parties, consignes).
- Sois fidÃ¨le au contenu : n'invente rien, ne rÃ©sume pas.
- Si un passage est difficile Ã  lire, fais de ton mieux et signale les incertitudes avec [illisible].

Pour les Ã©lÃ©ments visuels (graphiques, schÃ©mas, figures, cartes, diagrammes) :
- DÃ©cris-les en dÃ©tail entre balises [FIGURE: ...]

Pour les tableaux, utilise la syntaxe Markdown de tableau.
Pour les formules mathÃ©matiques, utilise la notation LaTeX entre $ ou $$.`

function loadImages (filenames: string[]): Array<{ base64: string; mimeType: string }> {
  const images: Array<{ base64: string; mimeType: string }> = []
  for (const f of filenames) {
    const path = join(IMG_DIR, f)
    if (!existsSync(path)) continue
    const buf = readFileSync(path)
    images.push({ base64: buf.toString('base64'), mimeType: 'image/jpeg' })
  }
  return images
}

async function fetchWithTimeout (url: string, body: any, headers: Record<string, string>, label: string, timeoutMs = 120000): Promise<any> {
  const controller = new AbortController()
  const timer = setTimeout(() => controller.abort(), timeoutMs)
  try {
    const res = await fetch(url, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json', ...headers },
      body: JSON.stringify(body),
      signal: controller.signal,
    })
    clearTimeout(timer)
    if (!res.ok) {
      const err = await res.text()
      throw new Error(`${label} ${res.status}: ${err.slice(0, 500)}`)
    }
    return res.json()
  } catch (err: any) {
    clearTimeout(timer)
    if (err.name === 'AbortError') throw new Error(`${label} timeout aprÃ¨s ${timeoutMs / 1000}s`)
    throw err
  }
}

async function transcribeWithGeminiModel (
  images: Array<{ base64: string; mimeType: string }>,
  label: string,
  geminiModel: string
): Promise<string> {
  const parts = images.map((img) => ({
    inline_data: { mime_type: img.mimeType, data: img.base64 },
  })).concat([{ text: TRANSCRIPTION_PROMPT } as any])

  const data = await fetchWithTimeout(
    `https://generativelanguage.googleapis.com/v1beta/models/${geminiModel}:generateContent?key=${ENV.GOOGLE_API_KEY}`,
    {
      contents: [{ parts }],
      generationConfig: { temperature: 0 },
      safetySettings: [
        { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE' },
      ],
    },
    {},
    `Gemini-Transcription/${label}`,
    90000 // 90s timeout
  )

  const text = data.candidates?.[0]?.content?.parts?.[0]?.text
  if (!text || text.length < 50) {
    const reason = data.candidates?.[0]?.finishReason || data.promptFeedback?.blockReason || '?'
    throw new Error(`Gemini rÃ©ponse vide pour ${label} (raison: ${reason})`)
  }

  return text
}

async function transcribeWithMistralOCR (
  images: Array<{ base64: string; mimeType: string }>,
  label: string
): Promise<string> {
  const pages: string[] = []
  for (let i = 0; i < images.length; i++) {
    console.log(`     Mistral OCR â€” page ${i + 1}/${images.length}`)
    const data = await fetchApi(
      'https://api.mistral.ai/v1/ocr',
      {
        model: 'mistral-ocr-latest',
        document: {
          type: 'image_url',
          image_url: `data:${images[i].mimeType};base64,${images[i].base64}`,
        },
      },
      { Authorization: `Bearer ${ENV.MISTRAL_API_KEY}` },
      `MistralOCR/${label}/page${i + 1}`
    )
    const pageText = data.pages?.map((p: any) => p.markdown).join('\n\n') ?? ''
    pages.push(pageText)
  }
  return pages.join('\n\n---\n\n')
}

async function transcribeImages (
  images: Array<{ base64: string; mimeType: string }>,
  label: string
): Promise<string> {
  console.log(`\n  ğŸ“¸ Transcription ${label} â€” ${images.length} images`)
  const t0 = performance.now()

  // Pipeline : Gemini Flash â†’ Gemini Pro â†’ Mistral OCR
  const pipeline = [
    { name: 'Gemini 3 Flash', fn: () => transcribeWithGeminiModel(images, label, 'gemini-3-flash-preview') },
    { name: 'Gemini 3 Pro', fn: () => transcribeWithGeminiModel(images, label, 'gemini-3-pro-preview') },
    { name: 'Mistral OCR', fn: () => transcribeWithMistralOCR(images, label) },
  ]

  for (const model of pipeline) {
    try {
      console.log(`     Essai ${model.name}...`)
      const text = await model.fn()
      const elapsed = ((performance.now() - t0) / 1000).toFixed(1)
      console.log(`  âœ… ${label} transcrit via ${model.name} â€” ${text.length} chars en ${elapsed}s`)
      return text
    } catch (err: any) {
      const isRecitation = err.message?.includes('RECITATION')
      console.log(`  âš ï¸ ${model.name} Ã©chouÃ©${isRecitation ? ' (RECITATION)' : ''}: ${err.message?.slice(0, 80)}`)
    }
  }

  throw new Error(`Tous les modÃ¨les de transcription ont Ã©chouÃ© pour ${label}`)
}

// â”€â”€â”€ Step 3 : Test barÃ¨me texte â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function extractValidJson (text: string): string | null {
  const starts: number[] = []
  for (let i = 0; i < text.length; i++) {
    if (text[i] === '{') starts.push(i)
  }
  for (const start of starts) {
    let depth = 0
    for (let i = start; i < text.length; i++) {
      if (text[i] === '{') depth++
      else if (text[i] === '}') {
        depth--
        if (depth === 0) {
          const candidate = text.slice(start, i + 1)
          if (candidate.length > 50) {
            try { JSON.parse(candidate); return candidate } catch {}
          }
          break
        }
      }
    }
  }
  return null
}

// â”€â”€ Anthropic â”€â”€â”€

async function testAnthropic (
  model: string,
  apiModel: string,
  prompt: string,
  isAdaptive: boolean
): Promise<{ raw: string; thinkingInfo: string }> {
  const body: any = {
    model: apiModel,
    max_tokens: isAdaptive ? 64000 : 8192,
    messages: [{ role: 'user', content: prompt }],
  }

  if (isAdaptive) {
    body.thinking = { type: 'adaptive' }
  } else {
    body.temperature = 0
  }

  const data = await fetchApi(
    'https://api.anthropic.com/v1/messages',
    body,
    { 'x-api-key': ENV.ANTHROPIC_API_KEY, 'anthropic-version': '2023-06-01' },
    `Anthropic/${model}`
  )

  const allTextBlocks = data.content?.filter((b: any) => b.type === 'text') ?? []
  const textBlock = allTextBlocks.length > 1
    ? allTextBlocks.reduce((best: any, b: any) => (b.text?.length ?? 0) > (best.text?.length ?? 0) ? b : best)
    : allTextBlocks[0]
  const thinkingBlocks = data.content?.filter((b: any) => b.type === 'thinking') ?? []

  const textContent = textBlock?.text ?? ''
  const thinkingContent = thinkingBlocks.map((b: any) => b.thinking ?? '').join('\n')
  const usage = data.usage ?? {}

  const thinkingInfo = [
    `stop=${data.stop_reason}`,
    `text=${textContent.length}ch`,
    `thinking=${thinkingContent.length}ch`,
    `input=${usage.input_tokens ?? '?'}`,
    `output=${usage.output_tokens ?? '?'}`,
  ].filter(Boolean).join(', ')

  if (textContent.length > 10) {
    return { raw: textContent, thinkingInfo }
  }

  // Fallback thinking
  if (isAdaptive && thinkingContent.length > 20) {
    const jsonMatch = thinkingContent.match(
      /\{[\s\S]*?("questions"|"sections"|"exercices"|"items"|"bareme"|"barÃ¨me"|"note_globale"|"total"|"total_points"|"total_gÃ©nÃ©ral"|"criteres"|"critÃ¨res"|"resultats"|"rÃ©sultats"|"corrections")[\s\S]*\}/
    )
    if (jsonMatch) {
      try { JSON.parse(jsonMatch[0]); return { raw: jsonMatch[0], thinkingInfo: thinkingInfo + ' [thinking-regex]' } } catch {}
    }
    const candidate = extractValidJson(thinkingContent)
    if (candidate) return { raw: candidate, thinkingInfo: thinkingInfo + ' [thinking-parse]' }
  }

  return { raw: textContent, thinkingInfo }
}

// â”€â”€ OpenAI â”€â”€â”€

async function testOpenAI (model: string, apiModel: string, prompt: string): Promise<string> {
  const body: any = {
    model: apiModel,
    messages: [{ role: 'user', content: prompt }],
    temperature: 0,
    seed: 42,
    response_format: { type: 'json_object' },
  }

  if (model === 'gpt-5-nano') {
    delete body.temperature
    delete body.seed
  }

  const data = await fetchApi(
    'https://api.openai.com/v1/chat/completions',
    body,
    { Authorization: `Bearer ${ENV.OPENAI_API_KEY}` },
    `OpenAI/${model}`
  )
  return data.choices[0].message.content
}

// â”€â”€ Gemini â”€â”€â”€

async function testGemini (model: string, apiModel: string, prompt: string): Promise<string> {
  const data = await fetchApi(
    `https://generativelanguage.googleapis.com/v1beta/models/${apiModel}:generateContent?key=${ENV.GOOGLE_API_KEY}`,
    {
      contents: [{ parts: [{ text: prompt }] }],
      generationConfig: { temperature: 0, responseMimeType: 'application/json' },
      safetySettings: [
        { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_NONE' },
        { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE' },
      ],
    },
    {},
    `Gemini/${model}`
  )

  if (!data.candidates?.[0]?.content?.parts?.[0]?.text) {
    throw new Error(`Gemini rÃ©ponse vide (${data.promptFeedback?.blockReason || data.candidates?.[0]?.finishReason || '?'})`)
  }
  return data.candidates[0].content.parts[0].text
}

// â”€â”€ Mistral â”€â”€â”€

async function testMistral (prompt: string): Promise<string> {
  const data = await fetchApi(
    'https://api.mistral.ai/v1/chat/completions',
    {
      model: 'mistral-large-2512',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0,
      response_format: { type: 'json_object' },
    },
    { Authorization: `Bearer ${ENV.MISTRAL_API_KEY}` },
    'Mistral/mistral-large'
  )
  return data.choices[0].message.content
}

// â”€â”€ DeepSeek â”€â”€â”€

async function testDeepSeek (prompt: string): Promise<string> {
  const data = await fetchApi(
    'https://api.deepseek.com/chat/completions',
    {
      model: 'deepseek-chat',
      messages: [{ role: 'user', content: prompt }],
      temperature: 0,
      seed: 42,
      response_format: { type: 'json_object' },
    },
    { Authorization: `Bearer ${ENV.DEEPSEEK_API_KEY}` },
    'DeepSeek'
  )
  return data.choices[0].message.content
}

// â”€â”€â”€ Main test runner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface ModelConfig {
  id: string
  provider: string
  apiModel: string
  apiKey: string
  isAdaptive?: boolean
}

const MODELS: ModelConfig[] = [
  { id: 'claude-opus-4-6', provider: 'anthropic', apiModel: 'claude-opus-4-6', apiKey: 'ANTHROPIC_API_KEY', isAdaptive: true },
  { id: 'claude-haiku-4-5', provider: 'anthropic', apiModel: 'claude-haiku-4-5-20251001', apiKey: 'ANTHROPIC_API_KEY' },
  { id: 'gemini-3-flash', provider: 'google', apiModel: 'gemini-3-flash-preview', apiKey: 'GOOGLE_API_KEY' },
  { id: 'gemini-3-pro', provider: 'google', apiModel: 'gemini-3-pro-preview', apiKey: 'GOOGLE_API_KEY' },
  { id: 'gpt-4o-mini', provider: 'openai', apiModel: 'gpt-4o-mini-2024-07-18', apiKey: 'OPENAI_API_KEY' },
  { id: 'gpt-5.2', provider: 'openai', apiModel: 'gpt-5.2-2025-12-11', apiKey: 'OPENAI_API_KEY' },
  { id: 'mistral-large', provider: 'mistral', apiModel: 'mistral-large-2512', apiKey: 'MISTRAL_API_KEY' },
  { id: 'deepseek-v3.2', provider: 'deepseek', apiModel: 'deepseek-chat', apiKey: 'DEEPSEEK_API_KEY' },
]

// â”€â”€â”€ Save detailed log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function saveLog (model: string, data: any) {
  const dir = join(process.cwd(), 'logs_appels_llm')
  if (!existsSync(dir)) mkdirSync(dir, { recursive: true })
  const ts = new Date().toISOString().replace(/:/g, '-').slice(0, 19)
  const path = join(dir, `${ts}_test-bareme-texte_${model}.json`)
  writeFileSync(path, JSON.stringify(data, null, 2))
  console.log(`  ğŸ’¾ Log: ${path}`)
}

async function runTest (m: ModelConfig, prompt: string) {
  const keyValue = ENV[m.apiKey as keyof typeof ENV]
  if (!keyValue) {
    console.log(`  â­ï¸  ${m.id} â€” API key manquante`)
    return
  }

  process.stdout.write(`  â³ ${m.id}...`)
  const t0 = performance.now()

  try {
    let raw: string
    let thinkingInfo = ''

    switch (m.provider) {
      case 'anthropic': {
        const result = await testAnthropic(m.id, m.apiModel, prompt, m.isAdaptive ?? false)
        raw = result.raw
        thinkingInfo = result.thinkingInfo
        break
      }
      case 'google':
        raw = await testGemini(m.id, m.apiModel, prompt)
        break
      case 'openai':
        raw = await testOpenAI(m.id, m.apiModel, prompt)
        break
      case 'mistral':
        raw = await testMistral(prompt)
        break
      case 'deepseek':
        raw = await testDeepSeek(prompt)
        break
      default:
        throw new Error(`Provider inconnu: ${m.provider}`)
    }

    const elapsed = performance.now() - t0

    // Save raw log
    saveLog(m.id, {
      model: m.id,
      mode: 'texte',
      raw_length: raw.length,
      raw_preview: raw.slice(0, 2000),
      thinkingInfo,
    })

    // Parse and normalize
    const parsed = robustJsonParse(raw)
    const bareme = normalizeBareme(parsed)

    const isOk = bareme.questions.length >= 3 && bareme.total > 0
      && bareme.questions[0].titre !== 'Item 1 â€” Ã€ complÃ©ter'

    // Affichage dÃ©taillÃ©
    console.log(`\n  ğŸ“Š ${m.id} â€” ${bareme.questions.length} questions, ${bareme.total} pts`)
    for (const q of bareme.questions.slice(0, 5)) {
      console.log(`     ${q.id}. ${q.titre} (${q.points} pts, ${q.criteres.length} critÃ¨res)`)
    }
    if (bareme.questions.length > 5) console.log(`     ... +${bareme.questions.length - 5} de plus`)

    console.log(` ${isOk ? 'âœ…' : 'âŒ'} ${(elapsed / 1000).toFixed(1)}s â€” ${bareme.questions.length} questions, ${bareme.total} pts`)

    results.push({
      model: m.id,
      status: isOk ? 'ok' : 'ko',
      time: elapsed,
      questions: bareme.questions.length,
      total: bareme.total,
      rawLength: raw.length,
      detail: isOk
        ? `${bareme.questions.length}q, ${bareme.total}pts â€” "${bareme.questions[0]?.titre?.slice(0, 40)}..."`
        : `Ã‰CHEC â€” ${bareme.questions.length}q, ${bareme.total}pts, raw=${raw.length}ch`,
      thinkingInfo,
    })
  } catch (err: any) {
    const elapsed = performance.now() - t0
    console.log(` âŒ ${(elapsed / 1000).toFixed(1)}s â€” ${err.message.slice(0, 100)}`)
    results.push({
      model: m.id,
      status: 'ko',
      time: elapsed,
      questions: 0,
      total: 0,
      rawLength: 0,
      detail: err.message.slice(0, 120),
    })
  }
}

function printResults () {
  console.log('\n' + 'â•'.repeat(120))
  console.log('  RÃ‰SULTATS â€” TEST BARÃˆME MODE TEXTE (pas d\'images)')
  console.log('  Attendu : 12 questions, 50 points')
  console.log('â•'.repeat(120))

  for (const r of results) {
    const icon = r.status === 'ok' ? 'âœ…' : 'âŒ'
    const time = `${(r.time / 1000).toFixed(1).padStart(6)}s`
    const model = r.model.padEnd(22)
    const qs = `${String(r.questions).padStart(3)}q`
    const pts = `${String(r.total).padStart(5)}pts`
    const raw = `${String(r.rawLength).padStart(6)}ch`

    const q12 = r.questions === 12 ? 'âœ“12q' : `âœ—${r.questions}q`
    const p50 = r.total === 50 ? 'âœ“50pts' : `âœ—${r.total}pts`

    console.log(`  ${icon} ${model} ${time}  ${qs}  ${pts}  ${raw}  [${q12} ${p50}]  ${r.detail.slice(0, 50)}`)
    if (r.thinkingInfo) {
      console.log(`     â””â”€ ${r.thinkingInfo}`)
    }
  }

  const ok = results.filter((r) => r.status === 'ok').length
  const ko = results.filter((r) => r.status === 'ko').length

  console.log(`\n  ${ok}/${results.length} OK  |  ${ko} KO`)

  // Tableau de conformitÃ© 12q/50pts
  console.log('\n  ğŸ“‹ CONFORMITÃ‰ 12 questions / 50 points :')
  for (const r of results) {
    const q = r.questions === 12 ? 'âœ…' : 'âŒ'
    const p = r.total === 50 ? 'âœ…' : 'âŒ'
    console.log(`     ${r.model.padEnd(22)} questions: ${q} (${r.questions})   points: ${p} (${r.total})`)
  }

  if (ko > 0) {
    console.log('\n  âŒ ERREURS :')
    for (const r of results.filter((r) => r.status === 'ko')) {
      console.log(`     ${r.model} â€” ${r.detail}`)
    }
  }
  console.log('â•'.repeat(120) + '\n')
}

// â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function main () {
  console.log('\nğŸ§ª Test barÃ¨me â€” mode TEXTE (transcription Gemini puis test tous LLM)')
  console.log(`   ${MODEL_FILTER ? `ModÃ¨le: ${MODEL_FILTER}` : 'Tous les modÃ¨les'}`)
  console.log(`   Skip transcription: ${SKIP_TRANSCRIPTION}\n`)

  let enonceText: string
  let corrigeText: string

  // â”€â”€ Ã‰tape 1 & 2 : Transcription â”€â”€

  if (SKIP_TRANSCRIPTION && existsSync(ENONCE_MD) && existsSync(CORRIGE_MD)) {
    console.log('ğŸ“‚ RÃ©utilisation des transcriptions existantes')
    enonceText = readFileSync(ENONCE_MD, 'utf-8')
    corrigeText = readFileSync(CORRIGE_MD, 'utf-8')
    console.log(`   enonce.md : ${enonceText.length} chars`)
    console.log(`   corrige.md : ${corrigeText.length} chars`)
  } else {
    console.log('ğŸ“ Ã‰tape 1/3 â€” Transcription des images via Gemini')

    const enonceImages = loadImages(['Ã©noncÃ©1.jpeg', 'Ã©noncÃ©2.jpeg', 'Ã©noncÃ©3.jpeg', 'Ã©noncÃ©4.jpeg'])
    const corrigeImages = loadImages(['corrigÃ©1.jpeg', 'corrigÃ©2.jpeg', 'corrigÃ©3.jpeg'])

    if (enonceImages.length === 0) {
      console.error('âŒ Aucune image d\'Ã©noncÃ© trouvÃ©e dans Ã©noncÃ©s/')
      process.exit(1)
    }

    // Transcription sÃ©quentielle (Ã©viter les conflits de rate limit Gemini)
    const enonceResult = await transcribeImages(enonceImages, 'Ã©noncÃ©')
    const corrigeResult = corrigeImages.length > 0
      ? await transcribeImages(corrigeImages, 'corrigÃ©')
      : ''

    enonceText = enonceResult
    corrigeText = corrigeResult

    // Sauvegarder les .md
    writeFileSync(ENONCE_MD, enonceText)
    console.log(`\n  ğŸ’¾ SauvegardÃ© : ${ENONCE_MD} (${enonceText.length} chars)`)

    if (corrigeText) {
      writeFileSync(CORRIGE_MD, corrigeText)
      console.log(`  ğŸ’¾ SauvegardÃ© : ${CORRIGE_MD} (${corrigeText.length} chars)`)
    }
  }

  // â”€â”€ Ã‰tape 3 : GÃ©nÃ©ration de barÃ¨me avec texte â”€â”€

  console.log('\nğŸ“ Ã‰tape 2/3 â€” GÃ©nÃ©ration de barÃ¨me sur tous les LLM (mode texte)')

  const prompt = getBaremePrompt(
    'Francais',
    '3Ã¨me',
    enonceText,
    corrigeText || undefined
  )

  console.log(`ğŸ“ Prompt: ${prompt.length} chars (corrigÃ©: ${corrigeText ? 'oui' : 'non'})\n`)

  const modelsToTest = MODEL_FILTER
    ? MODELS.filter((m) => m.id === MODEL_FILTER)
    : MODELS

  for (const m of modelsToTest) {
    await runTest(m, prompt)
  }

  // â”€â”€ Ã‰tape 3 : RÃ©sultats â”€â”€

  console.log('\nğŸ“ Ã‰tape 3/3 â€” RÃ©sultats')
  printResults()
  process.exit(results.some((r) => r.status === 'ko') ? 1 : 0)
}

main()
